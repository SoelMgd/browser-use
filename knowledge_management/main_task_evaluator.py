"""
Main script for task execution and evaluation with Browser-Use.

This script:
1. Executes a task with Browser-Use
2. Evaluates the result with an LLM evaluator
3. In case of failure, retries with the guide generated by the evaluator
4. Saves navigation graphs and success plans
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple, Literal
from urllib.parse import urlparse

# Add project path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from dotenv import load_dotenv

# Browser-Use imports
from browser_use.agent.service import Agent
from browser_use.browser import BrowserProfile, BrowserSession
from browser_use.llm import ChatAnthropic, ChatOpenAI
from browser_use.llm.messages import UserMessage

# Local imports
from knowledge_management.utils.history_parser import load_history_from_file, history_to_llm_messages, save_all_screenshots
from knowledge_management.utils.llm_response_parser import parse_llm_evaluation_response, ParsedLLMResponse
from knowledge_management.utils.plan_rag_manager import PlanRAGManager
from knowledge_management.utils.navigation_graph_manager import NavigationGraphManager
from knowledge_management.utils.guide_generator import GuideGenerator
from knowledge_management.prompts.eval_generation_prompts import SYSTEM_PROMPT_EVAL

# Logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()


class TaskEvaluator:
    """Main class for task evaluation and improvement"""
    
    def __init__(self, max_attempts: int = 3, llm_provider: Literal["anthropic", "openai"] = "anthropic"):
        self.max_attempts = max_attempts
        self.llm_provider = llm_provider
        
        # Vérifier que la clé API appropriée est définie
        if llm_provider == "anthropic":
            self.api_key = os.getenv('ANTHROPIC_API_KEY')
            if not self.api_key:
                raise ValueError("ANTHROPIC_API_KEY not defined in environment variables")
        elif llm_provider == "openai":
            self.api_key = os.getenv('OPENAI_API_KEY')
            if not self.api_key:
                raise ValueError("OPENAI_API_KEY not defined in environment variables")
        else:
            raise ValueError(f"Fournisseur LLM non reconnu: {llm_provider}")
        
        # Initialize Browser-Use
        self.browser_session = BrowserSession(
            browser_profile=BrowserProfile(
                headless=False,  # True in production
                minimum_wait_page_load_time=3,
                maximum_wait_page_load_time=10,
                viewport={'width': 1280, 'height': 1100},
                user_data_dir='~/.config/browseruse/profiles/default',
            )
        )
        
        # Initialize LLM for Browser-Use
        if llm_provider == "anthropic":
            self.browser_llm = ChatAnthropic(
                model="claude-sonnet-4-20250514",
            )
        else:  # openai
            self.browser_llm = ChatOpenAI(
                model="gpt-4o",
            )
        
        # Initialize evaluator LLM
        if llm_provider == "anthropic":
            self.evaluator_llm = ChatAnthropic(
                model="claude-sonnet-4-20250514",
                api_key=self.api_key,
                max_tokens=6000,
                temperature=0.1
            )
        else:  # openai
            self.evaluator_llm = ChatOpenAI(
                model="gpt-4o",
                api_key=self.api_key,
                max_tokens=6000,
                temperature=0.1
            )
        
        # Save paths
        self.navigation_graphs_dir = Path(__file__).parent / "navigation_graphs"
        self.plans_dir = Path(__file__).parent / "plans"
        self.screenshots_dir = Path(__file__).parent / "screenshots"
        self.tmp_dir = Path(__file__).parent.parent.parent / "tmp"
        
        # Create directories if needed
        self.navigation_graphs_dir.mkdir(exist_ok=True)
        self.plans_dir.mkdir(exist_ok=True)
        self.screenshots_dir.mkdir(exist_ok=True)
        self.tmp_dir.mkdir(exist_ok=True)
        
        self.rag_manager = PlanRAGManager() # Initialize RAG manager for plans
        self.nav_manager = NavigationGraphManager(llm_provider=llm_provider) # Initialize navigation graph manager
        self.guide_generator = GuideGenerator(llm_provider=llm_provider) # Initialize optimized guide generator
        
        logger.info(f"🎯 TaskEvaluator initialisé avec {llm_provider}")
    
    def _generate_task_id(self, task: str) -> str:
        """Generate a unique ID for the task"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        task_hash = hash(task) % 10000
        return f"task_{timestamp}_{task_hash}"
    
    async def _save_navigation_graph(self, navigation_graph: dict, website_url: str = None):
        """
        Save navigation graph using NavigationGraphManager
        """
        try:
            # Use NavigationGraphManager to save
            success = await self.nav_manager.save_navigation_graph(navigation_graph, website_url)
            if not success:
                logger.warning("⚠️ Failed to save navigation graph")
        except Exception as e:
            logger.error(f"❌ Error saving navigation graph: {e}")
    
    def _save_plans(self, task_id: str, guide: dict):
        """
        Save successful plan (now a dictionary of guides)
        
        Args:
            task_id: Task ID
            guide: Dictionary of guides with titles as keys
        """
        if not guide:
            logger.warning("⚠️ No guide to save")
            return
        
        # Save to file system (compatibility)
        filename = f"{task_id}_successful_plan.txt"
        filepath = self.plans_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for title, content in guide.items():
                f.write(f"## {title}\n\n{content}\n\n")
        
        logger.info(f"✅ Successful plan saved: {filepath}")
        
        # Save each guide in RAG system
        success = self.rag_manager.store_successful_plan(guide, task_id)
        if success:
            logger.info(f"💾 All guides stored in RAG system")
        else:
            logger.warning(f"⚠️ Failed to store some guides in RAG")
    
    def _get_rag_plans_context(self, task_title: Optional[str]) -> str:
        """
        Get RAG plans context for a given task title
        
        Args:
            task_title: Task title to search for (can be None)
            
        Returns:
            Formatted context from similar plans
        """
        if not task_title:
            logger.warning("⚠️ No task title available for RAG search.")
            return "No task title available for RAG search."
        
        try:
            similar_plans = self.rag_manager.find_similar_plans(task_title, top_k=10)
            if similar_plans:
                logger.info(f"🔍 Found {len(similar_plans)} similar plans")
                return self.rag_manager.build_context_from_similar_plans(similar_plans)
            else:
                logger.warning("⚠️ No similar successful plans found in RAG database.")
                return "No similar successful plans found in RAG database."
        except Exception as e:
            logger.warning(f"⚠️ Error retrieving RAG plans: {e}")
            return "Error retrieving RAG plans."
    
    def _get_navigation_graph_context(self, website_url: str) -> str:
        """
        Get navigation graph context for a given website
        
        Args:
            website_url: Website URL to search for
            
        Returns:
            Formatted context from navigation graphs
        """
        try:
            graphs = self.nav_manager.find_navigation_graphs_for_website(website_url)
            if graphs:
                return self.nav_manager.build_navigation_context(graphs, max_graphs=3)
            else:
                return "No previous navigation patterns available for this website."
        except Exception as e:
            logger.warning(f"⚠️ Error retrieving navigation graphs: {e}")
            return "Error retrieving navigation patterns."
    
    def _build_failure_recommendations_context(self, verdict: str, failure_guide: str) -> str:
        """
        Build context from failure recommendations
        
        Args:
            verdict: Evaluation verdict
            failure_guide: Failure guide from evaluator
            
        Returns:
            Formatted context for previous attempt
        """
        return f"""## A previous user tried this task and an evaluator provided feedback:

**Verdict:**
{verdict}

**Failure Guide (recommendations for next attempt):**
{failure_guide}

Use this information to understand what was tried before and avoid repeating unsuccessful approaches."""
    
    async def _evaluate_task_execution(self, history_file: str, task: str, report: str) -> ParsedLLMResponse:
        """Evaluate task execution with evaluator LLM"""
        try:
            # Load history
            history_data = load_history_from_file(history_file)
            
            # Convert to LLM messages
            llm_messages = history_to_llm_messages(history_data)
            
            # Create system message with task
            from browser_use.llm.messages import SystemMessage
            enhanced_system_prompt = f"""{SYSTEM_PROMPT_EVAL}

## The user try to reach this goal:
{task}

Please evaluate the user trajectory for this goal."""
            system_message = SystemMessage(content=enhanced_system_prompt)

            # Create user message with verdict
            if report:
                verdict_message = "The user also provided his own report of his task. This is useful to understand is trajectory: \n"
                verdict_message += report
                user_message = UserMessage(content=verdict_message)
                llm_messages.append(user_message)
            
            all_messages = [system_message] + llm_messages
            
            # Send to evaluator LLM
            logger.info("🔍 Evaluation in progress...")
            response = await self.evaluator_llm.ainvoke(all_messages)
            
            # Parse response
            parsed_response = parse_llm_evaluation_response(response.completion)
            
            logger.info(f"📋 Evaluation completed - Status: {parsed_response.task_label}")
            return parsed_response
            
        except Exception as e:
            logger.error(f"❌ Error during evaluation: {e}")
            raise
    
    async def _execute_task(self, task: str, enhanced_prompt: Optional[str] = None) -> Tuple[str, str]:
        """Execute a task with Browser-Use"""
        try:
            # Create agent with custom message context if provided
            agent_kwargs = {
                'task': task,
                'llm': self.browser_llm,
                'browser_session': self.browser_session,
                'validate_output': True,
                'enable_memory': False,
            }
            
            if enhanced_prompt:
                agent_kwargs['message_context'] = enhanced_prompt
            
            agent = Agent(**agent_kwargs)
            
            # Execute task
            logger.info("🚀 Executing task...")
            history = await agent.run(max_steps=25)

            logger.info(f"History final result: {history.final_result()}")
            logger.info(f"History is_done: {history.is_done()}")
            
            # Save history
            history_file = self.tmp_dir / "history.json"
            history.save_to_file(str(history_file))
            
            return str(history_file), history.final_result()
            
        except Exception as e:
            logger.error(f"❌ Error during task execution: {e}")
            raise
    
    async def run_task_with_evaluation(self, task: str, website_url: str) -> dict:
        """
        Execute a task with evaluation and iterative improvement
        
        Args:
            task: The task to execute
            website_url: The website URL where the task should be executed
            
        Returns:
            dict: Execution results
        """
        task_id = self._generate_task_id(task)
        logger.info(f"🎯 Starting task execution: {task_id}")
        logger.info(f"📝 Task: {task}")
        logger.info(f"🌐 Website: {website_url}")
        
        results = {
            'task_id': task_id,
            'task': task,
            'website_url': website_url,
            'attempts': [],
            'final_status': None,
            'successful_plan': None
        }
        
        current_failure_guide = None
        current_task_title = task
        current_website_url = website_url  # Use provided website_url
        current_verdict = None
        
        optimized_guide = None # Initialize empty optimized guide
        
        for attempt in range(1, self.max_attempts + 1):
            logger.info(f"\n🔄 Attempt {attempt}/{self.max_attempts}")
            
            try:

                # Subsequent attempts: use context from previous failed attempt
                logger.info("🎯 Generating optimized guide based on previous attempt context...")
                
                # Get contexts for subsequent attempts
                rag_context = self._get_rag_plans_context(current_task_title)
                nav_context = self._get_navigation_graph_context(current_website_url)
                
                # Build failure context
                failure_context = ""
                if current_verdict and current_failure_guide:
                    failure_context = self._build_failure_recommendations_context(current_verdict, current_failure_guide)
                
                optimized_guide = await self.guide_generator.generate_optimized_guide(
                    task=task,
                    website_url=current_website_url,
                    rag_plans_context=rag_context,
                    navigation_graph_context=nav_context,
                    previous_guide_context=failure_context,
                    attempt_count=attempt - 1
                )
                
                logger.info(f"🎯 Optimized guide generated: {optimized_guide}")
                # Build message context with optimized guide
                message_context = None
                if optimized_guide:
                    message_context = f"""## An external evaluator would like you to try this approach that could be helpful for the task:

{optimized_guide}

Follow this guide to complete the task efficiently and avoid common pitfalls."""
                
                # Execute task
                history_file, report = await self._execute_task(task, message_context)
                
                # Evaluate result
                evaluation = await self._evaluate_task_execution(history_file, task, report)
                
                # Save navigation graph with aggregation
                await self._save_navigation_graph(evaluation.navigation_graph, evaluation.website_url)
                
                # Save screenshots
                screenshots = self._save_screenshots(task_id, attempt, history_file)
                
                # Update metadata for next attempts
                current_task_title = evaluation.task_title
                # Update website URL if evaluator found a different one, otherwise keep the original
                if evaluation.website_url and evaluation.website_url != current_website_url:
                    logger.info(f"🌐 Website URL updated from {current_website_url} to {evaluation.website_url}")
                    current_website_url = evaluation.website_url
                current_navigation_graph = evaluation.navigation_graph
                current_verdict = evaluation.verdict
                
                # Record attempt results
                attempt_result = {
                    'attempt_number': attempt,
                    'verdict': evaluation.verdict,
                    'guide': evaluation.guide,
                    'failure_guide': evaluation.failure_guide,
                    'optimized_guide': optimized_guide,
                    'website_url': evaluation.website_url,
                    'task_title': evaluation.task_title,
                    'navigation_graph_file': f"{task_id}_attempt_{attempt}_graph.json",
                    'screenshots': screenshots
                }
                results['attempts'].append(attempt_result)
                
                logger.info(f"📊 Attempt {attempt} - Status: {evaluation.task_label}")
                logger.info(f"Verdict: {evaluation.verdict}")

                
                # If success, save plan in RAG and finish
                if evaluation.task_label == 'SUCCESS':
                    logger.info("✅ Task completed successfully!")
                    
                    # Save to file system (compatibility)
                    self._save_plans(task_id, evaluation.guide)
                    
                    results['final_status'] = 'SUCCESS'
                    results['successful_plan'] = evaluation.guide
                    break
                
                # If failure, use failure_guide for next attempt
                elif evaluation.task_label == 'FAILURE':
                    current_failure_guide = evaluation.failure_guide
                    self._save_plans(task_id, evaluation.guide)
                    logger.info("⚠️ Failure detected, failure_guide updated for next attempt")
                    
                # If impossible, stop
                elif evaluation.task_label == 'IMPOSSIBLE':
                    logger.info("❌ Task impossible to accomplish")
                    results['final_status'] = 'IMPOSSIBLE'
                    break
                
            except Exception as e:
                logger.error(f"❌ Error during attempt {attempt}: {e}")
                attempt_result = {
                    'attempt_number': attempt,
                    'status': 'ERROR',
                    'error': str(e)
                }
                results['attempts'].append(attempt_result)
        
        # If all attempts failed
        if not results['final_status']:
            results['final_status'] = 'FAILURE_AFTER_MAX_ATTEMPTS'
            logger.warning("⚠️ Failure after maximum attempts")
        
        return results

    def _save_screenshots(self, task_id: str, attempt: int, history_file: str):
        """Save attempt screenshots"""
        try:
            # Load history
            history_data = load_history_from_file(history_file)
            
            # Create directory for this attempt
            screenshots_dir = self.screenshots_dir / f"{task_id}_attempt_{attempt}"
            screenshots_dir.mkdir(exist_ok=True)
            
            # Save all screenshots
            saved_files = save_all_screenshots(history_data, str(screenshots_dir))
            
            logger.info(f"📸 Screenshots saved: {len(saved_files)} images in {screenshots_dir}")
            return saved_files
            
        except Exception as e:
            logger.error(f"❌ Error saving screenshots: {e}")
            return []


async def main():
    """Main function"""

    CREDENTIALS = """ To login, use the following credentials: {
  email: 'agentbenchmark6@gmail.com',
  password: 'Agent123456!',
  authtype: 'Google',
}"""
    
    # Configuration
    WEBSITE_URL = "https://www.crunchbase.com"
    TASK = """
    Log in to Crunchbase and create a new list titled ""Tech Unicorns,"" then add the companies Uber, Airbnb, and Stripe to the list.
    Only use https://www.crunchbase.com to achieve the task. Don't go to any other site. The task is achievable with just navigation from this site."""

    TASK = TASK + CREDENTIALS
    
    try:
        # Create evaluator
        evaluator = TaskEvaluator(max_attempts=3)
        
        # Execute task with evaluation
        results = await evaluator.run_task_with_evaluation(TASK, WEBSITE_URL)
        
        # Display final results
        print("\n" + "="*80)
        print("🎯 FINAL RESULTS")
        print("="*80)
        print(f"Task ID: {results['task_id']}")
        print(f"Final status: {results['final_status']}")
        print(f"Number of attempts: {len(results['attempts'])}")
        
        if results['successful_plan']:
            print(f"\n📋 Successful plan saved in: {evaluator.plans_dir}")
        
        print(f"\n📊 Navigation graphs saved in: {evaluator.navigation_graphs_dir}")
        print(f"📸 Screenshots saved in: {evaluator.screenshots_dir}")
        
        # Display attempt details
        print(f"\n📝 Attempt details:")
        for attempt in results['attempts']:
            print(f"  Attempt {attempt['attempt_number']}: {attempt.get('status', 'COMPLETED')}")
            if 'verdict' in attempt:
                print(f"    Verdict: {attempt['verdict']}...")
            if 'screenshots' in attempt and attempt['screenshots']:
                print(f"    Screenshots: {len(attempt['screenshots'])} images saved")
        
        # Display RAG statistics
        print(f"\n📚 RAG system statistics:")
        rag_stats = evaluator.rag_manager.get_plans_statistics()
        print(f"  Stored plans: {rag_stats['total_plans']}")
        print(f"  Unique task titles: {rag_stats['unique_task_titles']}")
        if rag_stats['task_titles']:
            print(f"  Task titles: {', '.join(rag_stats['task_titles'][:5])}...")

    except Exception as e:
        logger.error(f"❌ Error in main: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(main()) 